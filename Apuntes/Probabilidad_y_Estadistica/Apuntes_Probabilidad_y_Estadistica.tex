% Copyright (c) 2013-02-15 Sosa Juan Cruz
%  Permission is granted to copy, distribute and/or modify this document
%   under the terms of the GNU Free Documentation License, Version 1.3
%   or any later version published by the Free Software Foundation;
%   with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
%   A copy of the license is included in the section entitled "GNU
%    Free Documentation License". 
\documentclass[a4paper,10pt]{article}
\textheight=25cm %Establece el largo del texto en cada página. El default es 19 cm. 
\textwidth=17cm %Establece el ancho del texto en cada página (en este caso, de 17 cm). El default es 14 cm. 
\topmargin=-1cm %Establece el margen superior. El default es de 3 cm, en este caso la instrucción sube el margen 1 cm hacia arriba. 
\oddsidemargin=0mm %Establece el margen izquierdo de la hoja. El default es de 4.5 cm; sin embargo, con sólo poner esta instrucción el margen queda en 2.5 cm. Si el parámetro es positivo se aumenta este margen y si es negativo disminuye. 
\parindent=0mm %elimina la sangría. 
\usepackage[utf8x]{inputenc} % Paquete de idioma que incluye escritura latina
\usepackage{amssymb}        % Paquete para símbolos matemáticos
\usepackage{amsmath}
\usepackage{graphicx}       % Paquete para insertar imágenes
\usepackage[colorlinks=true,linkcolor=black,urlcolor=black]{hyperref} 
\usepackage{bookmark}       % Índice por Secciones en el PDF
\usepackage[spanish,es-nolists]{babel}
\title{ Apuntes de Probabilidad y Estadística }
%\setcounter{secnumdepth}{3}
\author{Juan Cruz Sosa \\ Guido Rajngewerc}
\date{23 de Mayo de 2013}
%%%%%%%%%%%%%%%% ESTO HACE QUE NO APAREZCA LA NUMERACIÓN EN LAS SECCIONES SALVO EN EL ÍNDICE %%%%%%%%%%%%%%%%%%%%%%
%\makeatletter													 %%
%\renewcommand\@seccntformat[1]{}										 %%
%\makeatother												         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../macros/matematica}
%%%%%%%%%%%%% ESTO HACE QUE SE IMPRIMAN LAS DEMOSTRACIONES
\versionlargatrue %%%%%%%%%% SI NO SE QUIEREN INPRIMIR LAS DEMOSTRACIONES SE PONE \versioncortafalse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\tableofcontents 
\clearpage
\section{Teoremas de Conteo}
\subsection{Conjuntos Independientes}


\ejemplo \\ \\
\begin{tabular}{ l  l }
   3 pares de zapatillas & $Z_1,\ Z_2,\ Z_3$
   \\ 4 pantalones & $P_1,\ P_2,\ P_3,\ P_4$
   \\ 6 remeras & $R_1,\ R_2,\ R_3,\ R_4,\ R_5,\ R_6$
 \end{tabular} \\

Formas de vestirse $Z_1,\ R_1,\ P_1,\cdots,Z_1,\ R_3,\ P_2,\cdots$: \ \ \  $3 \cdot 4 \cdot 6 = 72$


\subsection{Permutaciones}

\ejemplo Contraseña de 10 letras usando \textit{QWERTYUIOP} sin repetir \\ 

\begin{center}
  \begin{tabular}[t]{ | l | c | c | c | c | c | c | c | c | c | c | }
    \hline
    Posición & 1° & 2° & 3° & 4° & 5° & 6° & 7° & 8° & 9° & 10° \\
    \hline Posibilidades & 10 & 9 & 8 & 7 & 6 & 5 & 4 & 3 & 2 & 1 \\
    \hline
  \end{tabular}
\end{center}

Las posibilidades totales son: $10 \cdot 9 \cdot 8 \cdot 7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2 \cdot 1 = 10!$ \\ \\

$P_n = n! \mbox{ con } n \geq 0$


\subsection{Variaciones}


\ejemplo Carrera con 8 corredores, 1°, 2° y 3° puesto. Cuantas posibilidades hay para esos 3 puestos.

\begin{center}
  \begin{tabular}[t]{ | l | c | c | c | }
    \hline
    Puesto & 1° & 2° & 3° \\
    \hline Posibilidades & 8 & 7 & 6 \\
    \hline
  \end{tabular}
\end{center}

Las posibilidades son: \ \ $8 \cdot 7 \cdot 6 = \frac{8 \cdot 7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2 \cdot 1}{5 \cdot 4 \cdot 3 \cdot 2 \cdot 1} = \frac{8!}{5!} = \frac{8!}{(8-3)!}$  \\ \\

$V_{n,k} = \frac {n!} {(n-k)!} \mbox{ con } 0 \leq k \leq n$


\subsection{Combinaciones}


\ejemplo 8 corredores, miro 3 del podio pero no los distingo

Las posibilidades son: \ \ $\frac{8!}{5!} \cdot \frac{1}{3!} = \frac{8!}{5! \cdot 3!} = \binom {8}{3} = \binom {8}{5}$ (A las variaciones le quito las permutaciones de los puestos ya que los considero indistinguibles). \\ \\

$C_{n,k} = \binom {n}{k} \mbox{ con } 0 \leq k \leq n$

\section{Probabilidad}

\subsection{Espacio muestral}
Un espacio muestral es el conjunto de todas las posibilidades de un experimento \\

\ejemplo Tirar dos veces moneda. $C = Cara$. $S = Seca$. Entonces el espacio muestral $S$ es \
$S = \left\lbrace CC, CS, SC, SS \right\rbrace = \left\lbrace ij \ con \ i,j \in \left\lbrace C, S \right\rbrace \right\rbrace$ \\

\subsection{Eventos}
Los eventos son subconjuntos del espacio muestral

\ejemplo 2 dados. $S = \left\lbrace (i,j) \ con \ i,j \in \left\lbrace 1,\dots,6 \right \rbrace \right\rbrace$. \
Un evento A podría ser $A = \{\ Los \ 2 \ dados \ son \ pares\} = {(2,2),(2,4),(2,6),(4,2),(4,4),(4,6),(6,2),(6,4),(6,6)}$ \\

%\subsection{Frecuencia relativa}
%Dado un evento A. Se realizan $n$ experimentos. La frecuencia relativa $f_r : S \longrightarrow %[0,1]$ \\ $f_r(A) = \frac{\# veces \ que \ sucedio \ A}{n}$

\subsection{Probabilidad}
$\Omega$ espacio muestral. $A$ evento.\\ 
$P : \Omega \longrightarrow [0,1]$ \\
$P(A) = \mbox{Probabilidad del evento A}$
es probabilidad si satisface los siguientes axiomas
\begin{itemize}
\item $P(A) \in [0,1] \forall A \in S$
\item $P(A \cup B) = P(A) + P(B)$ si $A \cap B = \emptyset$
\item $P(\Omega) = 1$
\item $P(\bigcup_{i=1}^{\infty} {A_i}) = \sum_{i=1}^{\infty} {P(A_i)}$ si \ $A_i \cap A_j = \emptyset \ \forall i \neq j$
\end{itemize}

\propiedades
\begin{itemize}
 \item $P(A^C) = 1-P(A)$ 
 \item $P(\emptyset) = 0$
 \item $A \subseteq B \Rightarrow P(A) \leq P(B)$
 \item $P(A \cup B) = P(A)+P(B)-P(A \cap B)$
 \item $P(\bigcup_{i=1}^{n} {A_i}) \leq \sum_{i=1}^{n} {P(A_i)}$
 \item \begin{equation} 	
   	\begin{split}
   	P \left( \bigcup_{i=1}^{n} {A_i} \right) = 
  		  \sum_{k=1}^{n} { { \left( -1 \right) }^{k+1} P \left( 	\bigcap_{i \in I \subseteq \left[ 1:n \right] \# I = k } {A_i} \right) }
   \end{split}
	\end{equation}
\end{itemize}
\ifversionlarga
\begin{demo}
	\begin{itemize}
		\item Como $\displaystyle\Omega = A \dot\cup {A^{C}} \Rightarrow 1=P(\Omega)=P(A \dot\cup {A^{C}})=P(A)+P(A^{C}) \Rightarrow P(A^C) = 1-P(A)$ 
		\item $P(\emptyset)=P(\Omega^{C})=1-P(\Omega)=1-1=0$
		\item $P(B)=P(A \dot\cup (B-A))=P(A)+P(B-A) \geq P(A)$ 
		\item $P(A \cup B) = P(A \dot\cup (B \cap A^{C})) = P(A) + P(B \cap A^{C}) = P(A) + P(B) - P(A \cap B)$\\ (pues $P(B) = P((A \cap B) \dot\cup (B \cap A^{C})) = P(A \cap B) + P(B \cap A^{C}) \Rightarrow P(B \cap A^{C}) = P(B) - P(A \cap B)$)
		\item por inducción. El caso base $n=1$ es trivial. Supongo que vale hasta $n$ y pruebo que esto implica que vale para $n+1$. \\
		$\displaystyle P\left(\bigcup_{i=1}^{n+1} {A_i}\right)=P\left(\left(\bigcup_{i=1}^{n} {A_i} \right) \bigcup A_{n+1}\right) = P\left(\bigcup_{i=1}^{n} {A_i} \right)+P\left(A_{n+1}\right)-P\left(\left(\bigcup_{i=1}^{n} {A_i} \right) \bigcap A_{n+1}\right) $ \\
		Por hipótesis inductiva $P\left(\bigcup_{i=1}^{n} {A_i}\right) \leq \sum_{i=1}^{n} {P(A_i)}$ y además $P\left(\left(\bigcup_{i=1}^{n} {A_i} \right) \bigcap A_{n+1}\right) \leq 0$. Entonces \\
		$\displaystyle P\left(\bigcup_{i=1}^{n+1} {A_i}\right) = P\left(\bigcup_{i=1}^{n} {A_i} \right)+P\left(A_{n+1}\right)-P\left(\left(\bigcup_{i=1}^{n} {A_i} \right) \bigcap A_{n+1}\right) \leq \left(\sum_{i=1}^{n} {P(A_i)}\right) + P\left(A_{n+1}\right) \leq \sum_{i=1}^{n+1} {P(A_i)}$ \\
		Entonces vale $\forall n \in \mathbb{N}$
		\item por inducción
	\end{itemize}
\end{demo} 
\else
\fi
\subsection{Probabilidad condicional}
\definicion Sean $A,B$ eventos, $P(B) > 0$. \\
La probabilidad condicional de A dado B (A dado que sucedió B) es. \\
$P(A \dado B ) = \frac{P(A \cap B)}{P(B)} = \frac{P(AB)}{P(B)}$ \\ \\
\observacion $P(\ \dado B)$ es una probabilidad en el espacio muestral $B$ \\ \\
\definicionA {Probabilidad total} Sea $\Omega$ espacio muestral.
\begin{itemize}
	\item $A_1,A_2,\dots$ partición de $\Omega$
	\item $\displaystyle A_i \cap A_j = \emptyset \ \forall i,j$ con $i \neq j$
	\item $\displaystyle \bigcup_{i} {A_i} = \Omega$
	\item $B$ evento tal que $B = \displaystyle \mathop{\dot\bigcup}_{i} {\left( B \cap A_i \right)}$
\end{itemize}
entonces \\
$\displaystyle P(B)=P \left( \displaystyle \mathop{\dot\bigcup}_{i} {\left( B \cap A_i \right)} \right)=\sum_{i} {P ( B \cap A_i )}=\sum_{i} {P(A_i)P(B \dado A_i)}$ \\ \\
\teoremaDe {Bayes} Sea $\Omega$ espacio muestral.$A_1,\dots,A_i,\dots$ partición de $\Omega$.\ $P(A_i) > 0$, $P(B) > 0$, $B$ evento \\ \\
$P(A_i \dado B) = \displaystyle \frac {P(B \dado A_i)P(A_i)} {\displaystyle\sum_{j=0}{P(B \dado A_j)P(A_j)}}$ \\ \\

\subsection{Independencia}

\definicion $A$ es independiente de $B$ o $A$ y $B$ son independientes si \\
$P(A \dado B) = P(A) \Leftrightarrow \frac{P(AB)}{P(B)} = P(A) \Leftrightarrow P(AB) = P(A)P(B)$ \\ 

\propiedad $A$ y $B$ son independientes $ \Rightarrow A$ y $B^C$ son independientes \\ 

\definicion $A,B,C$ se dicen famila de eventos independientes si y solo si 

\begin{itemize}
	\item $P(ABC) = P(A)P(B)P(C)$
	\item $P(AB) = P(A)P(B)$
	\item $P(BC) = P(B)P(C)$
	\item $P(AC) = P(A)P(C)$
\end{itemize}

\propiedad $A,B,C$ son independientes $ \Rightarrow A$ es independiente de cualquier evento formado en $B$ y $C$ \\

\propiedad $(A_j, \mbox{ con } j \in J)$ ($J$ conjunto finito o numerable) son una familia de eventos independientes si $\forall I \subseteq J$ ($I$ conjunto finito o numerable de índices) \\
$\displaystyle P \left( \bigcap_{i \in I} {A_i} \right) = \prod_{i \in I} {P(A_i)}$

\section{Variables aleatorias}

\subsection{Variable aleatoria discretas}

\definicion Llamamos variable aleatoria a cualquier función $X : \Omega \longrightarrow \mathbb{R}$ (se usa para particionar el espacio muestral \\

\notacion En lugar de escribir $X^{-1}(x) = \left\lbrace s \in \Omega : X(S) = x \right\rbrace$ escribimos $\left\lbrace X = x \right\rbrace$ y en lugar de $X^{1}(R)$ escribimos $\left\lbrace x \in R \right\rbrace = \left\lbrace s \in \Omega : X(s) \in R \right\rbrace$ \\

\notacion Im(X) = Rango(X) = Rg(X) \\

\notacion $P(\left\{X = x \right\}) = P(X = x) = p_X(x)$ \\

\notacion $P(X < x) = F_X(x)$ se le dice función acumada de X\\ 

\definicion $E(X) = \sum_{i>1} x_i \cdot P(X = x_i)$, $x_i \in Rg(X)$ \\

\propiedadA {linalidad de la Esperanza} $E(aX+bY) = aE(X) + bE(Y)$ donde $X,Y$ son v.a, y $a,b$ constantes \\

\propiedadA {del Estadístico inconciente} $g:\mathbb{R} \Rightarrow \mathbb{R}$. $E(g(X)) = \sum_{i>1} g(x_i) \cdot P(X =x_i)$ \\ 

\definicion $V(X) = E({(X-E(X))}^{2}) = \sum_{i>1} {{(x_i-E(X))}^{2} P(X = x_i)}$, $x_i \in Rg(X)$ \\

\propiedad $V(X) = E(X^2)-E^2(X)$ \\

\propiedad $V(aX+b) = a^2V(X)$ 

\definicion El momento de orden de una v.a X es $E(X^k)$ si existe
\begin{itemize}
	\item $E(X) = \mu$ primer momento (de localización)
	\item $E(X^2) = \mu^2+\sigma^2$ segundo momento (de dispersión)
	\item $E(X^3)$ tercer momento (de asimetría)
	\item $E(X^4)$ cuarto momento (de cúrtosis)
	\item etc...
\end{itemize}

\definicionA {funcion Generadora de momentos} La función generadora de momentos(FGM) \\
	$M_X : \mathbb{R} \Rightarrow \mathbb{R}$ \\
	$M_X(t) = E(e^{tx})$ si existe para algún intervalo $t \in (-h,h)$ (que contenga a 0) \\

\propiedad Los momentos determinan la FGM \\ 

\teorema X tiene Funcion generadora de momentos $M_x$  entonces \\
$E(X^n)=\frac{\partial^n}{\partial t^n} {M_X(t)} = E \left( \frac{\partial^n}{\partial t^n} {e^{tx}}\right) \mid_{t=0} = E({X^n})$ 

\subsection{Distribución de Bernoulli}
\distribucion $X=Be(p)$ \\ \\
\rangoVar $Rg(X)=\left\lbrace 0,1 \right\rbrace$ \\
\end{document}
